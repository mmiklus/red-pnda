{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample tutorial to read json file using Apache Spark\n",
    "With a SparkSession or sqlContext, applications can create DataFrames from an existing RDD, from a Hive table, or from Spark data sources.\n",
    "\n",
    "As an example, the following creates a DataFrame based on the content of a JSON file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "testJsonData = sqlContext.read.json(\"/data/year=2017/month=7/day=4/hour=14/dump.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- host_ip: string (nullable = true)\n",
      " |-- rawdata: string (nullable = true)\n",
      " |-- src: string (nullable = true)\n",
      " |-- timestamp: long (nullable = true)\n",
      " |-- year: integer (nullable = true)\n",
      " |-- month: integer (nullable = true)\n",
      " |-- day: integer (nullable = true)\n",
      " |-- hour: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# print the schema\n",
    "testJsonData.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+---+-------------+----+-----+---+----+\n",
      "|host_ip|             rawdata|src|    timestamp|year|month|day|hour|\n",
      "+-------+--------------------+---+-------------+----+-----+---+----+\n",
      "|my_ipv6|python-random-618...|ESC|1499170050873|2017|    7|  4|  14|\n",
      "|my_ipv6|python-random-259...|ESC|1499170051727|2017|    7|  4|  14|\n",
      "|my_ipv6|python-random-998...|ESC|1499170130638|2017|    7|  4|  14|\n",
      "|my_ipv6|python-random-120...|ESC|1499170131380|2017|    7|  4|  14|\n",
      "|my_ipv6|python-random-393...|ESC|1499170131993|2017|    7|  4|  14|\n",
      "|my_ipv6|python-random-775...|ESC|1499170132597|2017|    7|  4|  14|\n",
      "|my_ipv6|python-random-282...|ESC|1499170133135|2017|    7|  4|  14|\n",
      "|my_ipv6|python-random-588...|ESC|1499170133706|2017|    7|  4|  14|\n",
      "+-------+--------------------+---+-------------+----+-----+---+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Displays the content of the DataFrame to stdout\n",
    "testJsonData.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Untyped Dataset Operations (aka DataFrame Operations)\n",
    "DataFrames provide a domain-specific language for structured data manipulation in Scala, Java, Python and R.\n",
    "\n",
    "DataFrames are just Dataset of Rows. These operations are also referred as “untyped transformations” in contrast to “typed transformations” come with strongly typed Scala/Java Datasets.\n",
    "\n",
    "Here we include some basic examples of structured data processing using Datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|             rawdata|\n",
      "+--------------------+\n",
      "|python-random-618...|\n",
      "|python-random-259...|\n",
      "|python-random-998...|\n",
      "|python-random-120...|\n",
      "|python-random-393...|\n",
      "|python-random-775...|\n",
      "|python-random-282...|\n",
      "|python-random-588...|\n",
      "+--------------------+\n",
      "\n",
      "+---+-----------+\n",
      "|src|(month + 1)|\n",
      "+---+-----------+\n",
      "|ESC|          8|\n",
      "|ESC|          8|\n",
      "|ESC|          8|\n",
      "|ESC|          8|\n",
      "|ESC|          8|\n",
      "|ESC|          8|\n",
      "|ESC|          8|\n",
      "|ESC|          8|\n",
      "+---+-----------+\n",
      "\n",
      "+-------+--------------------+---+-------------+----+-----+---+----+\n",
      "|host_ip|             rawdata|src|    timestamp|year|month|day|hour|\n",
      "+-------+--------------------+---+-------------+----+-----+---+----+\n",
      "|my_ipv6|python-random-393...|ESC|1499170131993|2017|    7|  4|  14|\n",
      "|my_ipv6|python-random-775...|ESC|1499170132597|2017|    7|  4|  14|\n",
      "|my_ipv6|python-random-282...|ESC|1499170133135|2017|    7|  4|  14|\n",
      "|my_ipv6|python-random-588...|ESC|1499170133706|2017|    7|  4|  14|\n",
      "+-------+--------------------+---+-------------+----+-----+---+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Select only the \"rawdata\" column\n",
    "testJsonData.select(\"rawdata\").show()\n",
    "\n",
    "# Select src and month, but increment only the month by 1\n",
    "testJsonData.select(testJsonData['src'], testJsonData['month'] + 1).show()\n",
    "\n",
    "# Select data where timestamp > 1499170131380\n",
    "testJsonData.filter(testJsonData['timestamp'] > 1499170131380).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running SQL Queries Programmatically\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+---+-------------+----+-----+---+----+\n",
      "|host_ip|             rawdata|src|    timestamp|year|month|day|hour|\n",
      "+-------+--------------------+---+-------------+----+-----+---+----+\n",
      "|my_ipv6|python-random-618...|ESC|1499170050873|2017|    7|  4|  14|\n",
      "|my_ipv6|python-random-259...|ESC|1499170051727|2017|    7|  4|  14|\n",
      "|my_ipv6|python-random-998...|ESC|1499170130638|2017|    7|  4|  14|\n",
      "|my_ipv6|python-random-120...|ESC|1499170131380|2017|    7|  4|  14|\n",
      "|my_ipv6|python-random-393...|ESC|1499170131993|2017|    7|  4|  14|\n",
      "|my_ipv6|python-random-775...|ESC|1499170132597|2017|    7|  4|  14|\n",
      "|my_ipv6|python-random-282...|ESC|1499170133135|2017|    7|  4|  14|\n",
      "|my_ipv6|python-random-588...|ESC|1499170133706|2017|    7|  4|  14|\n",
      "+-------+--------------------+---+-------------+----+-----+---+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Register the DataFrame as a SQL temporary view\n",
    "sqlContext.registerDataFrameAsTable(testJsonData, \"data\")\n",
    "\n",
    "sqlDF = sqlContext.sql(\"SELECT * FROM data\")\n",
    "sqlDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Further reading\n",
    "[https://spark.apache.org/docs/1.6.1/sql-programming-guide.html](https://spark.apache.org/docs/1.6.1/sql-programming-guide.html)\n",
    "\n",
    "[https://docs.databricks.com/spark/latest/spark-sql/index.html](https://docs.databricks.com/spark/latest/spark-sql/index.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
